{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Primer_encuentro_con_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBU6yLVGIDbr+NWFLSdNLK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMr5TYdo0Dr6",
        "outputId": "5e284c87-7347-42b5-ee04-1a29e8c93db5"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"cess_esp\")\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kL3aZI41Vcv",
        "outputId": "ccdfbbd2-36d2-43cd-f5be-b9e16bcb911d"
      },
      "source": [
        "corpus =nltk.corpus.cess_esp.sents()\n",
        "print(len(corpus))\n",
        "print(corpus)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6030\n",
            "['*0*', 'Destacó', 'que', '\"', 'por_fin', '\"', 'el', 'PP', 'ha', 'ganado', 'en', 'la', 'circunscripción', 'de', 'Málaga', 'tanto', 'en', 'las', 'elecciones', 'generales', 'como', 'en', 'las', 'autonómicas', 'y', 'en', 'el', 'Senado', ',', 'hecho', 'que', '*0*', 'achacó', 'entre', 'otros', 'factores', '\"', 'al', 'trabajo', 'de', 'los', 'alcaldes', ',', 'dando', 'ejemplo', '\"', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4Tf2TNb1zxp",
        "outputId": "e51a1799-7d18-4e65-8a8e-af35f201241d"
      },
      "source": [
        "#Transformamos la lista de listas en una sola lista para poder usar los tokens\n",
        "flatten = [w for l in corpus for w in l]\n",
        "print(len(flatten))\n",
        "print(flatten[:10])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "192685\n",
            "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqR-Y6VIeZd9"
      },
      "source": [
        "Patrones de busqueda con la funcion re.search().\n",
        "Con esto ^ al princio busca las palabras que empiecen con eso que pusiste.\n",
        "Cuando pones esto $ al final, busca las palabras que terminen con lo que pongas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kq5c75begYo",
        "outputId": "e1dbcbba-8d4b-4962-8941-7ce37ce5cfbf"
      },
      "source": [
        "#Busca palabras que tengan di en cualquier parte de ellas\n",
        "# arr = [w for w in flatten if re.search(\"di\", w)]\n",
        "# print(arr)\n",
        "\n",
        "#Busca palabras que empiecen con di\n",
        "# arr = [w for w in flatten if re.search(\"^di\", w)]\n",
        "# print(arr)\n",
        "\n",
        "#Busca palabras que terminen en di\n",
        "# arr = [w for w in flatten if re.search(\"di$\", w)]\n",
        "# print(arr)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Radio_Euskadi', 'Emannuelle_Gagliardi', 'Martine_Ceccaldi', 'Ceccaldi', 'Rimoldi', 'Gustavo_Lombardi', 'Lombardi', 'Christian_Fittipaldi', 'Moscardi', 'Euskadi', 'Romano_Prodi', 'Prodi', 'Euskadi', 'Claudio_Aranzadi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNR8eBKff7Br"
      },
      "source": [
        "Rangos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpGHznTGgDr1",
        "outputId": "92c11cfe-c20a-48f4-d353-8e80188a9e28"
      },
      "source": [
        "arr = [w for w in flatten if re.search(\"^[aci]\", w)]\n",
        "print(arr[:10])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['anunció', 'compra', 'creada', 'central', 'a', 'construcción', 'al', 'como', 'combustible', 'central']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMCcbU9PhBwa"
      },
      "source": [
        "#Clausuras\n",
        "# * Se repite 0 o mas veces\n",
        "# + Se repite 1 o mas veces\n",
        "\n",
        "#Se repite 1 o mas veces si en la palabra que estamos buscando\n",
        "# arr = [w for w in flatten if re.search(\"^(si)+\", w)]\n",
        "# print(arr)\n",
        "\n",
        "#Se repite 0 o mas veces si en la palabra que estamos buscando\n",
        "# arr = [w for w in flatten if re.search(\"^(si)*\", w)]\n",
        "# print(arr)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWtRt-wwjzex"
      },
      "source": [
        "Tokenizar texto de forma sencilla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyyCW5M5j3Yh"
      },
      "source": [
        "texto = \"\"\" Hola buen dia, este es un texto de pruba para ver como tokenizar cualquier texto que tengas,\n",
        "en este caso lo estoy escribiendo aqui, podrias hacerlo en un archivo txt (eso creo) pero por \n",
        "el momento lo haremos aqui ...\"\"\""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI59fA1uki-M",
        "outputId": "de591ee7-3488-4316-a67c-a92528c2c89a"
      },
      "source": [
        "#Caso 1: Tokenizacion por espacio vacios\n",
        "print(re.split(r\" \", texto))\n",
        "#Es un tokenizador basico que no es tan util"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Hola', 'buen', 'dia,', 'este', 'es', 'un', 'texto', 'de', 'pruba', 'para', 'ver', 'como', 'tokenizar', 'cualquier', 'texto', 'que', 'tengas,\\nen', 'este', 'caso', 'lo', 'estoy', 'escribiendo', 'aqui,', 'podrias', 'hacerlo', 'en', 'un', 'archivo', 'txt', '(eso', 'creo)', 'pero', 'por', '\\nel', 'momento', 'lo', 'haremos', 'aqui', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRpnsG2Ek-RC",
        "outputId": "6bb8d73d-7ed1-4207-e120-3f15376710a5"
      },
      "source": [
        "#Caso 2: Tokenizacion con expreciones regulares\n",
        "print(re.split(r\"[ \\t\\n]+\", texto))\n",
        "#Es un poco mejor pero se puede mejorar todavia mas"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Hola', 'buen', 'dia,', 'este', 'es', 'un', 'texto', 'de', 'pruba', 'para', 'ver', 'como', 'tokenizar', 'cualquier', 'texto', 'que', 'tengas,', 'en', 'este', 'caso', 'lo', 'estoy', 'escribiendo', 'aqui,', 'podrias', 'hacerlo', 'en', 'un', 'archivo', 'txt', '(eso', 'creo)', 'pero', 'por', 'el', 'momento', 'lo', 'haremos', 'aqui', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HBPYq3glaLZ",
        "outputId": "059b2405-db3b-44f0-877c-6fcbb19eb269"
      },
      "source": [
        "#Caso 3: Expresiones regulares del caso 2 pero mejoradas\n",
        "print(re.split(r\"[ \\W\\t\\n]+\", texto))\n",
        "#Ya esta mucho mejor nuestro tokenizador, por ejemplo ya quita los parentesis y deja solo las palabras"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Hola', 'buen', 'dia', 'este', 'es', 'un', 'texto', 'de', 'pruba', 'para', 'ver', 'como', 'tokenizar', 'cualquier', 'texto', 'que', 'tengas', 'en', 'este', 'caso', 'lo', 'estoy', 'escribiendo', 'aqui', 'podrias', 'hacerlo', 'en', 'un', 'archivo', 'txt', 'eso', 'creo', 'pero', 'por', 'el', 'momento', 'lo', 'haremos', 'aqui', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1uETsfIl6eL"
      },
      "source": [
        "Tokenizar con NLTK (Para textos mas complejos y con mas caracteres especiales)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqICtlyGmESG",
        "outputId": "7e2728bb-494c-45b1-9c36-b678d65f039d"
      },
      "source": [
        "texto = \"En los E.U esa postal vale $15.50 ...\"\n",
        "#Si usamos el tokenizador anterior no sirve para este texto\n",
        "print(re.split(r\"[ \\W\\t\\n]+\", texto))\n",
        "#Nos separa EU cuando deberian estar juntos y tambien separa 15.50 \n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['En', 'los', 'E', 'U', 'esa', 'postal', 'vale', '15', '50', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdjrJuiQmkt8"
      },
      "source": [
        "#Para hacer mejor nuestra tokenizacion utilizaremos esto\n",
        "pattern = r'''(?x)                    # Flag para iniciar el modo verbose\n",
        "              (?:[A-Z]\\.)+            # Hace match con abreviaciones como U.S.A.\n",
        "              | \\w+(?:-\\w+)*          # Hace match con palabras que pueden tener un guión interno\n",
        "              | \\$?\\d+(?:\\.\\d+)?%?    # Hace match con dinero o porcentajes como $15.5 o 100%\n",
        "              | \\.\\.\\.                # Hace match con puntos suspensivos\n",
        "              | [][.,;\"'?():-_`]      # Hace match con signos de puntuación\n",
        "'''"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qgrqe30nIII",
        "outputId": "89d8dc28-0dc6-49ad-be04-c66aa3c98807"
      },
      "source": [
        "#Ahora usamos la funcion de NLTK\n",
        "nltk.regexp_tokenize(texto, pattern)\n",
        "#Ahora ya separa las palabras justo como las necesitamos"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['En', 'los', 'E.', 'U', 'esa', 'postal', 'vale', '$15.50', '...']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}